  # ============================================================================
  # SCRIPT: Split GBIF MGRS Data Cubes by Ramsar Site Polygons
  #
  # DESCRIPTION:
  # This script processes a large CSV file of biodiversity data referenced by
  # 100m MGRS grids. It reads the data in manageable chunks, converts MGRS
  # coordinates to spatial points, and intersects them with a collection of
  # Ramsar site polygons (provided as individual .wkt files). The output is a
  # set of CSV files, one for each Ramsar site, containing only the
  # biodiversity records that fall within that site's boundary.
  #
  # REQUIREMENTS:
  # - R packages: sf, data.table, dplyr, mgrs
  # - A directory of Ramsar site polygons in .wkt format, as generated by
  #   your previous script.
  # - A large CSV file (the "continental data cube") with a column containing
  #   100m MGRS grid identifiers.
  #
  # AUTHOR: Gemini
  # DATE: 2023-10-27
  # ============================================================================

# --- 0. SETUP: LOAD LIBRARIES AND CONFIGURE PATHS ---

# Ensure required packages are installed, or install them
  packages <- c("sf", "data.table", "dplyr", "stringi", "mgrs") # Add all packages your script uses
  for (p in packages) {
    if (!requireNamespace(p, quietly = TRUE)) {
      install.packages(p)
    }
    library(p, character.only = TRUE)
  }

message("--- Libraries loaded successfully. ---")

# --- PLEASE CONFIGURE THESE VARIABLES ---

# Set the working directory to where your project files are located.
setwd("E:/R_projects/b3gbi")

# Path to the large continental biodiversity data cube CSV file.
# !! IMPORTANT !! Update this to your actual file path.
# Example: "C:/Users/You/Desktop/data/continental_cube_europe.csv"
continental_cube_csv_path <- "inst/extdata/continental_gbif_data/northamerica_100m_mgrs_all.csv" # <-- PLEASE UPDATE

# Name of the column in your CSV that contains the 100m MGRS grid ID.
# !! IMPORTANT !! Update this to match your CSV's column name.
mgrs_column_name <- "mgrscellcode" # <-- PLEASE UPDATE

# Path to the base directory where your country-specific folders of
# .wkt files for Ramsar sites are stored.
ramsar_wkt_base_dir <- "inst/extdata/ramsar_sites_wkt" # <-- PLEASE UPDATE

# Path to the directory where the output CSV files for each Ramsar site
# will be saved. This directory will be created if it doesn't exist.
output_dir <- "output/ramsar_site_data_100m_northamerica" # <-- PLEASE UPDATE

# Number of rows to read from the large CSV file at a time.
# Adjust this based on your computer's RAM. Lower if you get memory errors.
chunk_size <- 10000000 # 10 million rows per chunk

# --- END OF CONFIGURATION ---

# Create the output directory if it doesn't exist
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
  message(paste("Created output directory:", output_dir))
}


# --- 1. LOAD AND PREPARE ALL RAMSAR SITE POLYGONS ---

message("\n--- Step 1: Loading and preparing Ramsar site polygons... ---")

# Recursively find all .wkt files in the specified directory
wkt_files <- list.files(ramsar_wkt_base_dir, pattern = "\\.wkt$", recursive = TRUE, full.names = TRUE)

message(paste("--- Found", length(wkt_files), ".wkt files in:", ramsar_wkt_base_dir, "---"))

if (length(wkt_files) == 0) {
  stop("FATAL ERROR: No .wkt files found in the specified directory: ", ramsar_wkt_base_dir,
       "\nPlease check the 'ramsar_wkt_base_dir' path.")
}

# Use a list to temporarily store sf objects for each site
ramsar_sf_list <- lapply(wkt_files, function(file_path) {
  tryCatch({
    # Read the WKT string from the file
    wkt_string <- readLines(file_path, warn = FALSE)

    # Convert WKT string to an sf geometry object, assuming WGS84
    geom <- st_as_sfc(wkt_string, crs = 4326)

    # Extract metadata from the filename (e.g., "site_1_Some_Name.wkt")
    filename <- basename(file_path)
    ramsar_site_id <- tools::file_path_sans_ext(filename)
    site_name_for_display <- ramsar_site_id
    country_name <- basename(dirname(file_path))


    # Create a one-row sf data frame for this site
    site_sf <- st_as_sf(data.frame(
      ramsar_site_id = ramsar_site_id,
      site_name = site_name_for_display,
      country_en = country_name,
      source_wkt_file = filename
    ), geom = geom)

    return(site_sf)
  }, error = function(e) {
    message(paste("  [Warning] Failed to process WKT file:", file_path, "-", e$message))
    return(NULL)
  })
})

# Combine all the individual site sf objects into one large sf data frame
# and remove any that failed to load (are NULL)
ramsar_sites_sf <- do.call(rbind, Filter(Negate(is.null), ramsar_sf_list))

if (is.null(ramsar_sites_sf) || nrow(ramsar_sites_sf) == 0) {
  stop("FATAL ERROR: Could not load any valid Ramsar site polygons. Script cannot continue.")
}

message(paste("--- Successfully loaded", nrow(ramsar_sites_sf), "Ramsar site polygons into a single spatial layer. ---"))

message("\n--- Unique Ramsar Site IDs loaded from WKT files: ---")
print(unique(ramsar_sites_sf$ramsar_site_id))
message("\n--- Unique Countries loaded from WKT file paths: ---")
print(unique(ramsar_sites_sf$country_en))

message("\n--- Details of first 5 loaded Ramsar sites (if available): ---")
print(head(ramsar_sites_sf[, c("ramsar_site_id", "site_name", "country_en", "source_wkt_file")], 5))

# --- Clean and validate the combined Ramsar geometries ---
# The s2 geometry engine is strict and can error on minor issues like duplicate
# vertices ('degenerate edges'). st_make_valid() cleans these issues to prevent
# errors during the spatial join.
message("\n--- Validating and cleaning Ramsar site geometries to prevent join errors... ---")
ramsar_sites_sf <- st_make_valid(ramsar_sites_sf)
message("--- Geometries validated. ---")



# --- 2. PROCESS THE LARGE BIODIVERSITY DATA CUBE IN CHUNKS ---

message("\n--- Step 2: Processing the large biodiversity CSV in chunks... ---")

if (!file.exists(continental_cube_csv_path)) {
  stop("FATAL ERROR: The continental data cube CSV file was not found at: ", continental_cube_csv_path)
}

# Get the header of the CSV to apply to each chunk
header <- names(fread(continental_cube_csv_path, nrows = 0))
if (!(mgrs_column_name %in% header)) {
  stop("FATAL ERROR: The MGRS column '", mgrs_column_name, "' was not found in the CSV file.")
}


# Initialize variables for the chunking loop
total_rows_processed <- 0
chunk_num <- 1
# first_write_flags <- list() # To track if we've written the header for each output file

repeat {

  withr::local_options(list(scipen = 999)) # Disable scientific notation so chunk_size can be read by head function
  message(paste("\n- Processing chunk", chunk_num, "(starting after row", total_rows_processed, ")..."))

  start_line <- total_rows_processed + 2
  cmd_string <- paste0("tail -n +", start_line, " ", shQuote(continental_cube_csv_path, type = "cmd"), " | head -n ", chunk_size)

  # Read a chunk of the data using data.table's fread for speed
  data_chunk <- tryCatch({
    # For the first chunk, we let fread detect the header.
    # For subsequent chunks, we skip the rows we've already processed and tell it there's no header.
    if (chunk_num == 1) {
      fread(continental_cube_csv_path,
            nrows = chunk_size,
            encoding = "UTF-8",
            showProgress = FALSE)
    } else {
      # fread(continental_cube_csv_path,
      #       skip = total_rows_processed + 1, # +1 because we skip the header row too
      fread(cmd = cmd_string,
            header = FALSE,
            col.names = header,
            encoding = "UTF-8",
            showProgress = FALSE)
    }
  }, error = function(e) {
    message("  [Error] Could not read data chunk: ", e$message)
    return(NULL)
  })


  if (is.null(data_chunk) || nrow(data_chunk) == 0) {
    message("- Reached the end of the CSV file.")
    break # Exit the loop if there's no more data
  }

  # --- 2a. Convert MGRS to Geographic Coordinates ---
  message(paste("  Converting", nrow(data_chunk), "MGRS codes to lat/lon points..."))

  # Keep only unique MGRS codes to avoid redundant conversions
  unique_mgrs_codes <- unique(data_chunk[[mgrs_column_name]])

  # Perform the conversion
  latlon_coords <- tryCatch({
    mgrs_to_latlng(unique_mgrs_codes)
  }, error = function(e){
    message("  [Warning] An error occurred during MGRS conversion. Some coordinates may be missing. Error: ", e$message)
    return(data.frame(mgrs=character(), lat=numeric(), lon=numeric()))
  })

  # --- 2b. Create an sf object from the points ---
  # Remove any rows where conversion failed (resulted in NA)
  valid_coords <- na.omit(latlon_coords)

  if(nrow(valid_coords) == 0) {
    message("  No valid MGRS codes could be converted in this chunk. Skipping.")
    total_rows_processed <- total_rows_processed + nrow(data_chunk)
    chunk_num <- chunk_num + 1
    next # Move to the next chunk
  }

  # Create an sf object from the valid coordinates
  points_sf <- st_as_sf(valid_coords, coords = c("lng", "lat"), crs = 4326)

  # --- 2c. Perform the Spatial Join ---
  message("  Performing spatial join to find points within Ramsar sites...")

  # Temporarily disable S2 (spherical geometry) to use the more robust
  # planar geometry engine (GEOS), which can resolve validation errors that
  # st_make_valid() doesn't catch.
  sf_use_s2(FALSE)

  # `st_join` with `left = FALSE` acts as an inner join, keeping only points
  # that intersect with a Ramsar polygon. This is very efficient.
  points_in_ramsar <- st_join(points_sf, ramsar_sites_sf, join = st_intersects, left = FALSE)

  # Re-enable S2 now that the critical operation is complete.
  sf_use_s2(TRUE)

  if (nrow(points_in_ramsar) == 0) {
    message("   No records from this chunk were found inside any Ramsar site.")
  } else {
    message(paste("   SUCCESS: Found", nrow(points_in_ramsar), "records within Ramsar sites in this chunk."))

    # Merge the original data from the chunk back to the spatial results
    # The join is based on the MGRS column, which is named 'mgrs' in `points_in_ramsar`
    # and `mgrs_column_name` in `data_chunk`.
    setnames(points_in_ramsar, "mgrs", mgrs_column_name)

    # Use data.table for a high-performance merge
    results_to_write_dt <- merge(
      as.data.table(points_in_ramsar),
      as.data.table(data_chunk),
      by = mgrs_column_name
    )

    # --- 2d. Write/Append Results to Site-Specific Files ---
    # The `ramsar_site_id` and `country_en` columns now indicate which site each point belongs to.
    # We split the data table by this combination to ensure unique files per site per country.
    # This is the crucial change to ensure correct country-specific splitting.
    split_by_site_country <- split(results_to_write_dt, by = c("ramsar_site_id", "country_en"))

    message(paste("   Attempting to write to", length(names(split_by_site_country)), "distinct output files for this chunk."))

    for(site_country_key in names(split_by_site_country)) {
      site_data <- split_by_site_country[[site_country_key]]

      # Parse the key to get site_id and country_name
      # The key generated by data.table::split with by=c("col1", "col2") is "col1.col2"
      key_parts <- strsplit(site_country_key, "\\.")[[1]]
      site_id <- key_parts[1]
      country_name <- key_parts[2] # Correctly extract country name from the split key

      # Get the sanitized site name for a more descriptive filename (from ramsar_sites_sf)
      # Filter ramsar_sites_sf by both site_id and country_name to ensure precise lookup
      site_info <- ramsar_sites_sf[ramsar_sites_sf$ramsar_site_id == site_id & ramsar_sites_sf$country_en == country_name, ]
      site_name <- site_info$site_name[1] # Take the first if multiple exist (shouldn't if IDs are truly unique per country)

      # Create country-specific subdirectory
      country_output_dir <- file.path(output_dir, country_name)
      if (!dir.exists(country_output_dir)) {
        dir.create(country_output_dir, recursive = TRUE)
        message(paste("    Created country output directory:", country_output_dir))
      }

      # Define the output filename within the country directory
      # Using site_name here for descriptive filenames as requested previously.
      output_filename <- file.path(country_output_dir, paste0(site_id, "_data.csv"))

      # Determine if the file already exists on disk
      file_exists_on_disk <- file.exists(output_filename)

      # Use data.table's fwrite for fast, efficient CSV writing
      fwrite(site_data,
             file = output_filename,
             append = file_exists_on_disk, # Append if the file already exists
             col.names = !file_exists_on_disk) # Write header ONLY if the file does NOT exist

      if(!file_exists_on_disk) {
        message(paste("     -> Creating new file and writing", nrow(site_data), "records to:", basename(output_filename)))
      } else {
        message(paste("     -> Appending", nrow(site_data), "records to:", basename(output_filename)))
      }

      # Check if this is the first time we are writing to this file
      # The flag needs to be unique per site_id AND country to prevent overwriting
      # file_flag_key <- paste0(site_id, "_", country_name)
      # is_first_write <- is.null(first_write_flags[[file_flag_key]])

      # # Use data.table's fwrite for fast, efficient CSV writing
      # fwrite(site_data,
      #        file = output_filename,
      #        append = !is_first_write, # Append if not the first write
      #        col.names = is_first_write) # Write header only on the first write

      # # Mark that we have now written to this file at least once
      # if(is_first_write) {
      #   first_write_flags[[file_flag_key]] <- TRUE
      #   message(paste("     -> Creating new file and writing", nrow(site_data), "records to:", basename(output_filename)))
      # } else {
      #   message(paste("     -> Appending", nrow(site_data), "records to:", basename(output_filename)))
      # }
    }
  }

  # Prepare for the next iteration
  total_rows_processed <- total_rows_processed + nrow(data_chunk)
  chunk_num <- chunk_num + 1
}

message("\n\n--- ALL PROCESSING COMPLETE ---")
message(paste("Output files have been saved in:", output_dir))
