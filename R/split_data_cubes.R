  # ============================================================================
  # SCRIPT: Split GBIF MGRS Data Cubes by Ramsar Site Polygons
  #
  # DESCRIPTION:
  # This script processes a large CSV file of biodiversity data referenced by
  # 100m MGRS grids. It reads the data in manageable chunks, converts MGRS
  # coordinates to spatial points, and intersects them with a collection of
  # Ramsar site polygons (provided as individual .wkt files). The output is a
  # set of CSV files, one for each Ramsar site, containing only the
  # biodiversity records that fall within that site's boundary.
  #
  # REQUIREMENTS:
  # - R packages: sf, data.table, dplyr, mgrs
  # - A directory of Ramsar site polygons in .wkt format, as generated by
  #   your previous script.
  # - A large CSV file (the "continental data cube") with a column containing
  #   100m MGRS grid identifiers.
  #
  # AUTHOR: Gemini
  # DATE: 2023-10-27
  # ============================================================================

# --- 0. SETUP: LOAD LIBRARIES AND CONFIGURE PATHS ---

# Ensure required packages are installed, or install them
# install.packages(c("sf", "data.table", "dplyr", "mgrs"))

library(sf)
library(data.table)
library(dplyr)
library(mgrs)

message("--- Libraries loaded successfully. ---")

# --- PLEASE CONFIGURE THESE VARIABLES ---

# Path to the large continental biodiversity data cube CSV file.
# !! IMPORTANT !! Update this to your actual file path.
# Example: "C:/Users/You/Desktop/data/continental_cube_europe.csv"
continental_cube_csv_path <- "inst/extdata/continental_gbif_data/asia_100m_mgrs_all.csv" # <-- PLEASE UPDATE

# Name of the column in your CSV that contains the 100m MGRS grid ID.
# !! IMPORTANT !! Update this to match your CSV's column name.
mgrs_column_name <- "mgrscellcode" # <-- PLEASE UPDATE

# Path to the base directory where your country-specific folders of
# .wkt files for Ramsar sites are stored.
ramsar_wkt_base_dir <- "inst/extdata/ramsar_sites_wkt" # <-- PLEASE UPDATE

# Path to the directory where the output CSV files for each Ramsar site
# will be saved. This directory will be created if it doesn't exist.
output_dir <- "output/ramsar_site_data_100m" # <-- PLEASE UPDATE

# Number of rows to read from the large CSV file at a time.
# Adjust this based on your computer's RAM. Lower if you get memory errors.
chunk_size <- 2000000

# --- END OF CONFIGURATION ---

# Create the output directory if it doesn't exist
if (!dir.exists(output_dir)) {
  dir.create(output_dir, recursive = TRUE)
  message(paste("Created output directory:", output_dir))
}


# --- 1. LOAD AND PREPARE ALL RAMSAR SITE POLYGONS ---

message("\n--- Step 1: Loading and preparing Ramsar site polygons... ---")

# Recursively find all .wkt files in the specified directory
wkt_files <- list.files(ramsar_wkt_base_dir, pattern = "\\.wkt$", recursive = TRUE, full.names = TRUE)

if (length(wkt_files) == 0) {
  stop("FATAL ERROR: No .wkt files found in the specified directory: ", ramsar_wkt_base_dir,
       "\nPlease check the 'ramsar_wkt_base_dir' path.")
}

# Use a list to temporarily store sf objects for each site
ramsar_sf_list <- lapply(wkt_files, function(file_path) {
  tryCatch({
    # Read the WKT string from the file
    wkt_string <- readLines(file_path, warn = FALSE)

    # Convert WKT string to an sf geometry object, assuming WGS84
    geom <- st_as_sfc(wkt_string, crs = 4326)

    # Extract metadata from the filename (e.g., "site_1_Some_Name.wkt")
    filename <- basename(file_path)
    ramsar_site_id <- tools::file_path_sans_ext(filename)
    site_name_for_display <- ramsar_site_id


    # Create a one-row sf data frame for this site
    site_sf <- st_as_sf(data.frame(
      ramsar_site_id = ramsar_site_id,
      site_name = site_name_for_display,
      source_wkt_file = filename
    ), geom = geom)

    return(site_sf)
  }, error = function(e) {
    message(paste("  [Warning] Failed to process WKT file:", file_path, "-", e$message))
    return(NULL)
  })
})

# Combine all the individual site sf objects into one large sf data frame
# and remove any that failed to load (are NULL)
ramsar_sites_sf <- do.call(rbind, Filter(Negate(is.null), ramsar_sf_list))

if (is.null(ramsar_sites_sf) || nrow(ramsar_sites_sf) == 0) {
  stop("FATAL ERROR: Could not load any valid Ramsar site polygons. Script cannot continue.")
}

message(paste("--- Successfully loaded", nrow(ramsar_sites_sf), "Ramsar site polygons into a single spatial layer. ---"))

# --- Clean and validate the combined Ramsar geometries ---
# The s2 geometry engine is strict and can error on minor issues like duplicate
# vertices ('degenerate edges'). st_make_valid() cleans these issues to prevent
# errors during the spatial join. This is the fix for the user's reported error.
message("\n--- Validating and cleaning Ramsar site geometries to prevent join errors... ---")
ramsar_sites_sf <- st_make_valid(ramsar_sites_sf)
message("--- Geometries validated. ---")



# --- 2. PROCESS THE LARGE BIODIVERSITY DATA CUBE IN CHUNKS ---

message("\n--- Step 2: Processing the large biodiversity CSV in chunks... ---")

if (!file.exists(continental_cube_csv_path)) {
  stop("FATAL ERROR: The continental data cube CSV file was not found at: ", continental_cube_csv_path)
}

# Get the header of the CSV to apply to each chunk
header <- names(fread(continental_cube_csv_path, nrows = 0))
if (!(mgrs_column_name %in% header)) {
  stop("FATAL ERROR: The MGRS column '", mgrs_column_name, "' was not found in the CSV file.")
}


# Initialize variables for the chunking loop
total_rows_processed <- 0
chunk_num <- 1
first_write_flags <- list() # To track if we've written the header for each output file

repeat {
  message(paste("\n- Processing chunk", chunk_num, "(starting after row", total_rows_processed, ")..."))

  # Read a chunk of the data using data.table's fread for speed
  data_chunk <- tryCatch({
    # For the first chunk, we let fread detect the header.
    # For subsequent chunks, we skip the rows we've already processed and tell it there's no header.
    if (chunk_num == 1) {
      fread(continental_cube_csv_path,
            nrows = chunk_size,
            encoding = "UTF-8",
            showProgress = FALSE)
    } else {
      fread(continental_cube_csv_path,
            skip = total_rows_processed + 1, # +1 because we skip the header row too
            nrows = chunk_size,
            header = FALSE,
            col.names = header,
            encoding = "UTF-8",
            showProgress = FALSE)
    }
  }, error = function(e) {
    message("  [Error] Could not read data chunk: ", e$message)
    return(NULL)
  })


  if (is.null(data_chunk) || nrow(data_chunk) == 0) {
    message("- Reached the end of the CSV file.")
    break # Exit the loop if there's no more data
  }

  # --- 2a. Convert MGRS to Geographic Coordinates ---
  message(paste("  Converting", nrow(data_chunk), "MGRS codes to lat/lon points..."))

  # Keep only unique MGRS codes to avoid redundant conversions
  unique_mgrs_codes <- unique(data_chunk[[mgrs_column_name]])

  # Perform the conversion
  latlon_coords <- tryCatch({
    mgrs_to_latlng(unique_mgrs_codes)
  }, error = function(e){
    message("  [Warning] An error occurred during MGRS conversion. Some coordinates may be missing. Error: ", e$message)
    return(data.frame(mgrs=character(), lat=numeric(), lon=numeric()))
  })

  # --- 2b. Create an sf object from the points ---
  # Remove any rows where conversion failed (resulted in NA)
  valid_coords <- na.omit(latlon_coords)

  if(nrow(valid_coords) == 0) {
    message("  No valid MGRS codes could be converted in this chunk. Skipping.")
    total_rows_processed <- total_rows_processed + nrow(data_chunk)
    chunk_num <- chunk_num + 1
    next # Move to the next chunk
  }

  # Create an sf object from the valid coordinates
  points_sf <- st_as_sf(valid_coords, coords = c("lng", "lat"), crs = 4326)

  # --- 2c. Perform the Spatial Join ---
  message("  Performing spatial join to find points within Ramsar sites...")

  # Temporarily disable S2 (spherical geometry) to use the more robust
  # planar geometry engine (GEOS), which can resolve validation errors that
  # st_make_valid() doesn't catch.
  sf_use_s2(FALSE)

  # `st_join` with `left = FALSE` acts as an inner join, keeping only points
  # that intersect with a Ramsar polygon. This is very efficient.
  points_in_ramsar <- st_join(points_sf, ramsar_sites_sf, join = st_intersects, left = FALSE)

  # Re-enable S2 now that the critical operation is complete.
  sf_use_s2(TRUE)

  if (nrow(points_in_ramsar) == 0) {
    message("  No records from this chunk were found inside any Ramsar site.")
  } else {
    message(paste("  SUCCESS: Found", nrow(points_in_ramsar), "records within Ramsar sites in this chunk."))

    # Merge the original data from the chunk back to the spatial results
    # The join is based on the MGRS column, which is named 'mgrs' in `points_in_ramsar`
    # and `mgrs_column_name` in `data_chunk`.
    setnames(points_in_ramsar, "mgrs", mgrs_column_name)

    # Use data.table for a high-performance merge
    results_to_write_dt <- merge(
      as.data.table(points_in_ramsar),
      as.data.table(data_chunk),
      by = mgrs_column_name
    )

    # --- 2d. Write/Append Results to Site-Specific Files ---
    # The `ramsar_site_id` column now indicates which site each point belongs to.
    # We split the data table by this ID and write each part to its respective file.
    split_by_site <- split(results_to_write_dt, by = "ramsar_site_id")

    for(site_id in names(split_by_site)) {
      site_data <- split_by_site[[site_id]]

      # Get the sanitized site name for a more descriptive filename
      site_info <- ramsar_sites_sf[ramsar_sites_sf$ramsar_site_id == site_id, ]
      site_name <- site_info$site_name[1]

      output_filename <- file.path(output_dir, paste0(site_id, "_data.csv"))

      # Check if this is the first time we are writing to this file
      is_first_write <- is.null(first_write_flags[[site_id]])

      # Use data.table's fwrite for fast, efficient CSV writing
      fwrite(site_data,
             file = output_filename,
             append = !is_first_write, # Append if not the first write
             col.names = is_first_write) # Write header only on the first write

      # Mark that we have now written to this file at least once
      if(is_first_write) {
        first_write_flags[[site_id]] <- TRUE
        message(paste("    -> Creating new file and writing", nrow(site_data), "records to:", basename(output_filename)))
      } else {
        message(paste("    -> Appending", nrow(site_data), "records to:", basename(output_filename)))
      }
    }
  }

  # Prepare for the next iteration
  total_rows_processed <- total_rows_processed + nrow(data_chunk)
  chunk_num <- chunk_num + 1
}

message("\n\n--- ALL PROCESSING COMPLETE ---")
message(paste("Output files have been saved in:", output_dir))
